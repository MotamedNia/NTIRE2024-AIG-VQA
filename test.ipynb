{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import timm\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "# from vit_pytorch.vivit import ViT\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    video_path = \"testing\"\n",
    "    captions_path = \".\"\n",
    "    batch_size = 1\n",
    "    num_workers = 4\n",
    "    head_lr = 1e-3\n",
    "    video_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    classification_encoder_lr = 1e-4\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 20\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    frame_size = 224\n",
    "    video_len = 16\n",
    "    # video_embedding = 1024\n",
    "    video_embedding = 400\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "# %%\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_data, tokenizer, transforms, video_len):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "        video_filenames = []\n",
    "        prompts = []\n",
    "        for vid_data in video_data:\n",
    "            raw_data = vid_data.split('|')\n",
    "            video_filenames.append(raw_data[0])\n",
    "            prompts.append(raw_data[1].replace('\\n',''))\n",
    "\n",
    "        self.video_filenames = video_filenames\n",
    "        self.captions = list(prompts)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(prompts), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "        self.video_len = video_len\n",
    "\n",
    "    def read_video_as_tensor(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frames = []\n",
    "        padd_frame = np.zeros((CFG.frame_size, CFG.frame_size, 3), dtype=np.uint8)\n",
    "\n",
    "        frame_idx = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            frame = cv2.resize(frame,(CFG.frame_size, CFG.frame_size))\n",
    "            frame_tensor = torch.from_numpy(frame)  # Convert to PyTorch tensor\n",
    "            frames.append(frame_tensor)\n",
    "            frame_idx += 1\n",
    "        while frame_idx < CFG.video_len:\n",
    "            frame_tensor = torch.from_numpy(padd_frame)  # Convert to PyTorch tensor\n",
    "            frames.append(frame_tensor)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        video_tensor = torch.stack(frames)  # Stack frames to create video tensor\n",
    "        return video_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "        video = self.read_video_as_tensor(f\"{CFG.video_path}/{self.video_filenames[idx]}\")\n",
    "        \n",
    "        item['video'] = torch.tensor(video).permute(3, 0, 1, 2).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        item['filename'] = self.video_filenames[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# %%\n",
    "class VideoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = models.video.r3d_18(pretrained=True)\n",
    "        \n",
    "        # # video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n",
    "        # for p in self.model.parameters():\n",
    "        #     p.requires_grad = trainable\n",
    "        # self.model = swin3d_t(pretrained=True)\n",
    "        # self.model = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "        # video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# %%\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "\n",
    "\n",
    "# %%\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        video_embedding=CFG.video_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.video_encoder = VideoEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=video_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.classification_model = torch.nn.Sequential( \n",
    "                torch.nn.Linear(in_features = 256, out_features = 1), \n",
    "                torch.nn.Sigmoid() \n",
    "            )\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        video_features = self.video_encoder(batch[\"video\"])\n",
    "        \n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        video_embeddings = self.image_projection(video_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "        \n",
    "\n",
    "        # output_linear = self.classification_model(video_embeddings) \n",
    "        \n",
    "        # Calculating the Loss\n",
    "        embeddings_similarity = (self.cos(video_embeddings, text_embeddings)+1)/2\n",
    "        vec_product = video_embeddings*text_embeddings\n",
    "        magnitude1 = torch.norm(video_embeddings)\n",
    "        magnitude2 = torch.norm(text_embeddings)\n",
    "        normalized_dot = vec_product / (magnitude1 * magnitude2)\n",
    "        output_linear = self.classification_model(normalized_dot) \n",
    "        \n",
    "        \n",
    "        output = (embeddings_similarity + (3*output_linear))/4\n",
    "        \n",
    "        return output\n",
    "\n",
    "def build_loaders(data, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        data,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "        video_len=CFG.video_len\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt','r') as f:\n",
    "    df_valid = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]/tmp/ipykernel_22831/1355389807.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['video'] = torch.tensor(video).permute(3, 0, 1, 2).float()\n",
      "/tmp/ipykernel_22831/1355389807.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['video'] = torch.tensor(video).permute(3, 0, 1, 2).float()\n",
      "/tmp/ipykernel_22831/1355389807.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['video'] = torch.tensor(video).permute(3, 0, 1, 2).float()\n",
      "/tmp/ipykernel_22831/1355389807.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['video'] = torch.tensor(video).permute(3, 0, 1, 2).float()\n",
      "100%|██████████| 2000/2000 [06:04<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "    # train_df, valid_df = make_train_valid_dfs()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "valid_loader = build_loaders(df_valid, tokenizer, mode=\"valid\")\n",
    "\n",
    "\n",
    "model = CLIPModel().to(CFG.device)\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "final_scores = \"\"\n",
    "start = time.time()\n",
    "for batch in tqdm_object:\n",
    "    batch_model = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\" and k != \"filename\"}\n",
    "    model_output = model(batch_model)\n",
    "    score = model_output.detach().cpu().numpy()[0][0]\n",
    "    filename = batch['filename'][0]\n",
    "    final_scores += filename+\",\"+str(score*100)+\"\\n\"\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output.txt\", \"w\")\n",
    "f.write(final_scores)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022273842930793764"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end-start)/2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: readme.txt (deflated 29%)\n",
      "updating: output.txt (deflated 59%)\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip readme.txt  output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
